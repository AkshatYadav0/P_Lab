{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e00ed732-c657-4db0-85b7-167907b382b1",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Notebook: Predicting Behaviour"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5842913-716a-49ff-8f46-2d5e5a175c6d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef11f2fb-5269-4c19-8c01-9006215208e9",
   "metadata": {},
   "source": [
    "**This notebook extends the predicting behaviour model described in the [paper](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1008943)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd10e05-5e39-4c34-897a-67151f7179e4",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d315492-15a6-46b8-8527-1e65f9712314",
   "metadata": {},
   "source": [
    "**Data provided is already preprocessed but needs to be converted in model usabale format**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3f6f36f2-a0bf-4460-bc88-4906ed055872",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e1556b2d-782b-41c5-ade7-86913ca18b6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['testretest', 'twomen', 'bridgeville', 'pockets', 'overcome', 'inception', 'socialnet', 'oceans', 'flower', 'hotel', 'garden', 'dreary', 'homealone', 'brokovich', 'starwars'])\n"
     ]
    }
   ],
   "source": [
    "with open('HCP_movie_watching.pkl','rb') as f:\n",
    "    TS = pickle.load(f)\n",
    "print(TS.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1742dc86-ee0d-4313-97ba-713b04ba6a73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 176, 84, 300)\n",
      "(176, 245, 300)\n",
      "(176, 222, 300)\n",
      "(176, 189, 300)\n",
      "(176, 65, 300)\n",
      "(176, 227, 300)\n",
      "(176, 260, 300)\n",
      "(176, 250, 300)\n",
      "(176, 181, 300)\n",
      "(176, 186, 300)\n",
      "(176, 205, 300)\n",
      "(176, 143, 300)\n",
      "(176, 233, 300)\n",
      "(176, 231, 300)\n",
      "(176, 256, 300)\n"
     ]
    }
   ],
   "source": [
    "for movie_name, ts in TS.items():\n",
    "    print(ts.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a97fed-96de-430c-b0f4-319d61271b96",
   "metadata": {},
   "source": [
    "### Behavioural Scores Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8b922258-2653-4f73-9bbd-0b8222e57162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Subject', 'PMAT24_A_CR', 'PicVocab_Unadj', 'NEOFAC_A', 'NEOFAC_O',\n",
      "       'NEOFAC_C', 'NEOFAC_N', 'NEOFAC_E'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "with open('HCP_behavioral_scores.pkl','rb') as f:\n",
    "    TS1 = pickle.load(f)\n",
    "    print(TS1.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6042c0ee-6c50-4428-8f1f-ced89989f834",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Subject</th>\n",
       "      <th>PMAT24_A_CR</th>\n",
       "      <th>PicVocab_Unadj</th>\n",
       "      <th>NEOFAC_A</th>\n",
       "      <th>NEOFAC_O</th>\n",
       "      <th>NEOFAC_C</th>\n",
       "      <th>NEOFAC_N</th>\n",
       "      <th>NEOFAC_E</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.851851</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.620690</td>\n",
       "      <td>0.612903</td>\n",
       "      <td>0.135135</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.001901</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.438968</td>\n",
       "      <td>0.291667</td>\n",
       "      <td>0.655172</td>\n",
       "      <td>0.677419</td>\n",
       "      <td>0.432432</td>\n",
       "      <td>0.387097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.002466</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>0.141904</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.413793</td>\n",
       "      <td>0.483871</td>\n",
       "      <td>0.216216</td>\n",
       "      <td>0.516129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.004255</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.574822</td>\n",
       "      <td>0.708333</td>\n",
       "      <td>0.655172</td>\n",
       "      <td>0.903226</td>\n",
       "      <td>0.189189</td>\n",
       "      <td>0.322581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.005939</td>\n",
       "      <td>0.941176</td>\n",
       "      <td>0.411379</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.517241</td>\n",
       "      <td>0.419355</td>\n",
       "      <td>0.243243</td>\n",
       "      <td>0.548387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>0.951130</td>\n",
       "      <td>0.411765</td>\n",
       "      <td>0.354675</td>\n",
       "      <td>0.708333</td>\n",
       "      <td>0.586207</td>\n",
       "      <td>0.741935</td>\n",
       "      <td>0.405405</td>\n",
       "      <td>0.774194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>0.959536</td>\n",
       "      <td>0.470588</td>\n",
       "      <td>0.490631</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.482759</td>\n",
       "      <td>0.161290</td>\n",
       "      <td>0.540541</td>\n",
       "      <td>0.193548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>0.968477</td>\n",
       "      <td>0.470588</td>\n",
       "      <td>0.134435</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.344828</td>\n",
       "      <td>0.612903</td>\n",
       "      <td>0.540541</td>\n",
       "      <td>0.548387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>0.973156</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>0.681923</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.724138</td>\n",
       "      <td>0.645161</td>\n",
       "      <td>0.756757</td>\n",
       "      <td>0.612903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.764706</td>\n",
       "      <td>0.230945</td>\n",
       "      <td>0.208333</td>\n",
       "      <td>0.517241</td>\n",
       "      <td>0.774194</td>\n",
       "      <td>0.432432</td>\n",
       "      <td>0.677419</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>176 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Subject  PMAT24_A_CR  PicVocab_Unadj  NEOFAC_A  NEOFAC_O  NEOFAC_C  \\\n",
       "0    0.000000     1.000000        0.851851  0.375000  0.620690  0.612903   \n",
       "1    0.001901     0.823529        0.438968  0.291667  0.655172  0.677419   \n",
       "2    0.002466     0.647059        0.141904  0.583333  0.413793  0.483871   \n",
       "3    0.004255     1.000000        0.574822  0.708333  0.655172  0.903226   \n",
       "4    0.005939     0.941176        0.411379  0.375000  0.517241  0.419355   \n",
       "..        ...          ...             ...       ...       ...       ...   \n",
       "171  0.951130     0.411765        0.354675  0.708333  0.586207  0.741935   \n",
       "172  0.959536     0.470588        0.490631  0.500000  0.482759  0.161290   \n",
       "173  0.968477     0.470588        0.134435  0.458333  0.344828  0.612903   \n",
       "174  0.973156     0.647059        0.681923  0.416667  0.724138  0.645161   \n",
       "175  1.000000     0.764706        0.230945  0.208333  0.517241  0.774194   \n",
       "\n",
       "     NEOFAC_N  NEOFAC_E  \n",
       "0    0.135135  0.000000  \n",
       "1    0.432432  0.387097  \n",
       "2    0.216216  0.516129  \n",
       "3    0.189189  0.322581  \n",
       "4    0.243243  0.548387  \n",
       "..        ...       ...  \n",
       "171  0.405405  0.774194  \n",
       "172  0.540541  0.193548  \n",
       "173  0.540541  0.548387  \n",
       "174  0.756757  0.612903  \n",
       "175  0.432432  0.677419  \n",
       "\n",
       "[176 rows x 8 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = MinMaxScaler()\n",
    "dict_ = {}\n",
    "for col,ts in TS1.items():\n",
    "    k = scaler.fit_transform(np.array(ts).reshape((-1,1))).reshape((-1,))\n",
    "    dict_[col] = k\n",
    "\n",
    "df = pd.DataFrame.from_dict(dict_)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187648f5-bbf9-4131-bfd9-b9522b6ea286",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Dataset organization\n",
    "`TS` is a dictionary with movie names as keys\n",
    "\n",
    "Value against each key is a numpy array of dimensions `[#participants, #time points, #ROIs]`\n",
    "\n",
    "Note that the testretest movie appears on all 4 runs for a participant, therefore the value has dimensions `[#runs, #participants, #time points, #ROIs]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "607d6a22-aa55-4a97-81d0-1298f5886666",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data import TensorDataset, DataLoader, ConcatDataset\n",
    "import random\n",
    "def get_data(behav_score , movie, seq_length,shuffle=False):\n",
    "    rel = {}\n",
    "    l  = 1\n",
    "    for score in df[behav_score]:\n",
    "        rel[l] = score \n",
    "        l += 1\n",
    "    \n",
    "    train_feature = []\n",
    "    test_feature  = []\n",
    "    train_target  = []\n",
    "    test_target   = []\n",
    "    for movie_name, ts in TS.items():\n",
    "        pep = 0\n",
    "        if movie_name == movie:\n",
    "            for i in ts:\n",
    "                pep += 1\n",
    "                if (pep <= 100):\n",
    "                    if i.shape[0]>seq_length:\n",
    "                        k = i[:seq_length][:]\n",
    "                        train_feature.append(k)\n",
    "                        train_target.append([rel[pep]])\n",
    "\n",
    "                        k = i[i.shape[0]-seq_length:][:]\n",
    "                        train_feature.append(k)\n",
    "                        train_target.append([rel[pep]])\n",
    "\n",
    "                    elif i.shape[0]<seq_length:\n",
    "                        k = [[0]*300]*seq_length\n",
    "                        k[seq_length-i.shape[0]:] = i\n",
    "                        train_feature.append(k)\n",
    "                        train_target.append([rel[pep]])\n",
    "                    else:\n",
    "                        if shuffle:\n",
    "                            np.random.shuffle(i)\n",
    "                            train_feature.append(i)\n",
    "                        else:\n",
    "                            train_feature.append(i)\n",
    "                        train_target.append([rel[pep]]*seq_length)\n",
    "\n",
    "                else:\n",
    "                    if i.shape[0]>seq_length:\n",
    "                        k = i[:seq_length][:]\n",
    "                        test_feature.append(k)\n",
    "                        test_target.append([rel[pep]]*seq_length)\n",
    "\n",
    "                        k = i[i.shape[0]-seq_length:][:]\n",
    "                        test_feature.append(k)\n",
    "                        test_target.append([rel[pep]]*seq_length)\n",
    "\n",
    "                    elif i.shape[0]<seq_length:\n",
    "                        k = [[0]*300]*seq_length\n",
    "                        k[seq_length-i.shape[0]:] = i\n",
    "                        test_feature.append(k)\n",
    "                        test_target.append([rel[pep]]*seq_length)\n",
    "                    else:\n",
    "                        test_feature.append(i)\n",
    "                        test_target.append([rel[pep]]*seq_length)\n",
    "            print(pep)\n",
    "        '''else:\n",
    "            for jj in ts:\n",
    "                pep = 0\n",
    "                for i in jj:\n",
    "                    pep += 1\n",
    "                    if (pep <= 100):\n",
    "                        if i.shape[0]>seq_length:\n",
    "                            k = i[:seq_length][:]\n",
    "                            train_feature.append(k)\n",
    "                            train_target.append(rel[pep])\n",
    "\n",
    "                            k = i[i.shape[0]-seq_length:][:]\n",
    "                            train_feature.append(k)\n",
    "                            train_target.append(rel[pep])\n",
    "\n",
    "                        elif i.shape[0]<seq_length:\n",
    "                            k = [[0]*300]*seq_length\n",
    "                            k[seq_length-i.shape[0]:] = i\n",
    "                            train_feature.append(k)\n",
    "                            train_target.append(rel[pep])\n",
    "                        else:\n",
    "                            train_feature.append(i)\n",
    "                            train_target.append(rel[pep])\n",
    "\n",
    "                    else:\n",
    "                        if i.shape[0]>seq_length:\n",
    "                            k = i[:seq_length][:]\n",
    "                            test_feature.append(k)\n",
    "                            test_target.append(rel[pep])\n",
    "\n",
    "                            k = i[i.shape[0]-seq_length:][:]\n",
    "                            test_feature.append(k)\n",
    "                            test_target.append(rel[pep])\n",
    "\n",
    "                        elif i.shape[0]<seq_length:\n",
    "                            k = [[0]*300]*seq_length\n",
    "                            k[seq_length-i.shape[0]:] = i\n",
    "                            test_feature.append(k)\n",
    "                            test_target.append(rel[pep])\n",
    "                        else:\n",
    "                            test_feature.append(i)\n",
    "                            test_target.append(rel[pep])\n",
    "                print(pep)'''\n",
    "    \n",
    "    train_data = TensorDataset(torch.from_numpy(np.array(train_feature)).float(),torch.from_numpy(np.array(train_target)).float())\n",
    "    test_data  = TensorDataset(torch.from_numpy(np.array(test_feature)).float(),torch.from_numpy(np.array(test_target)).float())\n",
    "    \n",
    "    \n",
    "    batch_size  = 15\n",
    "    valid_data  = 0.30\n",
    "    t_train     = len(train_data)\n",
    "    data_no     = list(range(t_train))\n",
    "    np.random.shuffle(data_no)\n",
    "    split_no    = int(np.ceil(valid_data*t_train))\n",
    "    train,valid = data_no[split_no:] + data_no[:5],data_no[:split_no]\n",
    "\n",
    "    train_sampler = SubsetRandomSampler(train)\n",
    "    valid_sampler = SubsetRandomSampler(valid)\n",
    "\n",
    "    train_loader  = DataLoader(train_data,batch_size=batch_size,sampler=train_sampler)\n",
    "    valid_loader  = DataLoader(train_data,sampler=valid_sampler,batch_size=batch_size)\n",
    "    test_loader   = DataLoader(test_data, batch_size=batch_size)#,shuffle = True)\n",
    "    return train_loader,valid_loader,test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2c547f-dd11-4881-8f6d-bf9a04787a6a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Training, Validation, Test\n",
    "\n",
    "With the data in required shape, The following shows the split into training, validation, and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "31cc2a9b-713e-4b6a-a469-944aae53384d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "176\n"
     ]
    }
   ],
   "source": [
    "batch_size  = 15\n",
    "shuffle = False\n",
    "cols = ['PMAT24_A_CR','PicVocab_Unadj','NEOFAC_A','NEOFAC_O','NEOFAC_C','NEOFAC_E']\n",
    "mov = ['testretest', 'twomen', 'bridgeville', 'pockets', 'overcome', 'inception', 'socialnet', 'oceans', 'flower', 'hotel', 'garden', 'dreary', 'homealone', 'brokovich', 'starwars']\n",
    "movie = 'starwars'\n",
    "seq_length  = TS[movie].shape[1]\n",
    "score = cols[0]#'PMAT24_A_CR'\n",
    "\n",
    "train_loader,valid_loader,test_loader = get_data(score,movie,seq_length,shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b8b16bb6-a738-453b-b643-614545dff6ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 2, 6)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader),len(valid_loader),len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0164e572-1cde-43d2-9962-8807168c9f1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([15, 256, 300])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iter(train_loader).next()[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "94466fed-c399-40ae-bcac-6531df7eb884",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b125b2a-bb8d-4ce6-9f0a-8e069a79df24",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068beeed-357c-4d28-a104-eeaac7407776",
   "metadata": {},
   "source": [
    "### Implementing the `Attention` Model\n",
    "\n",
    "The following figures shows the idea behind it\n",
    "\n",
    "\n",
    "<img src=\"attntion1.png\" align=\"center\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "52bc285c-cd82-4567-b0c1-278785fda899",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_dim, seq_len, bias=False, **kwargs):\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "        \n",
    "        self.supports_masking = True\n",
    "        self.bias       = bias\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.seq_len    = seq_len\n",
    "        \n",
    "        weight = torch.zeros(hidden_dim, 1)\n",
    "        nn.init.kaiming_uniform_(weight)\n",
    "        \n",
    "        context = torch.zeros(seq_len,seq_len)\n",
    "        nn.init.kaiming_uniform_(context)\n",
    "        \n",
    "        self.context = nn.Parameter(context)\n",
    "        self.weight  = nn.Parameter(weight)\n",
    "        \n",
    "        if bias:\n",
    "            self.b = nn.Parameter(torch.zeros(seq_len))\n",
    "        \n",
    "    def forward(self, x,mask=None):\n",
    "        #print(x.shape,x.contiguous().view(-1, self.hidden_dim).shape)\n",
    "        eij = torch.mm(x.contiguous().view(-1, self.hidden_dim ), self.weight).view(-1,self.seq_len)\n",
    "        if self.bias:\n",
    "            eij = eij + self.b    \n",
    "        eij = torch.tanh(eij)\n",
    "        #print(eij.shape)\n",
    "        eij = torch.mm(eij,self.context)\n",
    "        #eij = torch.nn.functional.normalize(eij,p=2.5,dim=1)\n",
    "        a = torch.exp(eij)\n",
    "        if mask is not None:\n",
    "            a = a * mask\n",
    "\n",
    "        a = a / (torch.sum(a, 1, keepdim=True) + 1e-10)\n",
    "        weighted_input = x * torch.unsqueeze(a, -1)\n",
    "        #weighted_input = torch.nn.functional.normalize(weighted_input,p=2.5,dim=0)\n",
    "        #x = torch.sum(weighted_input, 2)\n",
    "        #x = x.view(batch_size,seq_length,1)\n",
    "        #print(torch.sum(weighted_input, 2).eq(weighted_input))\n",
    "        return weighted_input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d18314-cd91-4a77-ac9e-a996e32f9f26",
   "metadata": {},
   "source": [
    "### `GRU Classifier` Model as described in the [paper](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1008943)\n",
    "<img src=\"gru.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "03f4c547-27bc-4fd2-891b-2e47de2cefca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU_RNN(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, output_dim,hidden_dim,seq_length,n_layers,att=True,drop_prob=0.000001):\n",
    "        super(GRU_RNN, self).__init__()\n",
    "\n",
    "        self.output_dim = output_dim\n",
    "        self.n_layers   = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.seq_length = seq_length\n",
    "        self.att        = att\n",
    "        \n",
    "        self.gru       = nn.GRU(input_dim,hidden_dim,num_layers=n_layers,dropout=drop_prob,batch_first=True)\n",
    "        self.linear    = nn.Linear(hidden_dim,output_dim)\n",
    "        if att:\n",
    "            self.attention = Attention(hidden_dim,seq_length)\n",
    "        \n",
    "        self.dropout   = nn.Dropout(0.3)\n",
    "        self.func      = nn.Softmax(dim = -1)\n",
    "\n",
    "    def forward(self, x, hidden,encoder_x=0):\n",
    "        x,hidden = self.gru(x,hidden)\n",
    "        if self.att:\n",
    "            x = self.attention(x)\n",
    "        else:\n",
    "            x = x#[:, -1, :]\n",
    "        #x = self.dropout(x)\n",
    "        x = self.linear(x)\n",
    "        sig_out = x#self.func(x)\n",
    "        \n",
    "        return sig_out, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device)\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782f2153-8768-40da-ad15-a1f3747840e4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "18c1b324-d71f-4e27-bb26-c677621a2ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs,train_loader,net,valid_loader,optimzer,criterion,att=True):\n",
    "    val_acc = []\n",
    "    tr_acc = []\n",
    "    \n",
    "    clip = 4 # gradient clipping\n",
    "\n",
    "    net.to(device)\n",
    "    net.train()\n",
    "    \n",
    "    valid_loss_min = np.Inf \n",
    "    \n",
    "    valid_losses = []\n",
    "    train_losses = []\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        num_correct = 0\n",
    "        h = net.init_hidden(batch_size)\n",
    "        train_loss = []\n",
    "        valid_loss = []\n",
    "        train_acc  = 0.0\n",
    "        valid_acc  = 0.0 \n",
    "        counter = 0\n",
    "        for inputs, labels in train_loader:\n",
    "            top_value = 0\n",
    "            counter += 1\n",
    "            #if counter == 65:\n",
    "            #    continue\n",
    "            #print(labels)\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            #print(labels)\n",
    "            h = h.data\n",
    "            net.zero_grad()\n",
    "\n",
    "            output, h = net(inputs, h)\n",
    "            #output = output[:, -1, :]\n",
    "            output = output.view(batch_size,-1)\n",
    "            #print(output.shape,labels.shape)#view(batch_size,-1)\n",
    "            \n",
    "            #print(output[0])\n",
    "            #break\n",
    "            #pred = output#torch.round(output.squeeze()) \n",
    "            #print(pred)\n",
    "            #top_value, top_index = torch.max(output,1)\n",
    "            #print(top_value)\n",
    "            #print(labels.shape)\n",
    "            #print(top_value, top_index)\n",
    "            correct_tensor = output.eq(labels.float())#.view_as(top_index))\n",
    "            correct = np.squeeze(correct_tensor.to('cpu').numpy())\n",
    "            num_correct += np.sum(correct)\n",
    "\n",
    "            loss = criterion(output, labels)\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss.append(loss.item())\n",
    "        tr_acc.append(num_correct/((len(train_loader)-1)*batch_size))\n",
    "\n",
    "\n",
    "\n",
    "        acc = 0.0\n",
    "        val_h = net.init_hidden(batch_size)\n",
    "        val_losses = []\n",
    "        net.eval()\n",
    "        num_correct = 0\n",
    "        v_c = 0\n",
    "        for inputs, labels in valid_loader:\n",
    "            top_value = 0\n",
    "            v_c += 1\n",
    "            if v_c==2:\n",
    "                continue\n",
    "            val_h = val_h.data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            output, val_h = net(inputs, val_h)\n",
    "            \n",
    "            #pred = torch.round(output.squeeze()) \n",
    "            #print(pred)\n",
    "            #top_value, top_index = torch.max(output,1)\n",
    "            output = output.view(batch_size,-1)\n",
    "            #output = output[:, -1, :]\n",
    "            #print(top_value.squeeze().shape,labels.shape)\n",
    "            correct_tensor = output.eq(labels.float())#.view_as(top_index))\n",
    "            correct = np.squeeze(correct_tensor.to('cpu').numpy())\n",
    "            num_correct += np.sum(correct)\n",
    "\n",
    "            val_loss = criterion(output,labels)\n",
    "            val_losses.append(val_loss.item())\n",
    "            \n",
    "            if val_loss.item() <= valid_loss_min:\n",
    "                print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min, val_loss.item()))\n",
    "                best_epoch = e\n",
    "                if att:\n",
    "                    torch.save(net.state_dict(), 'RNN_GRU_Att.pt')\n",
    "                else:\n",
    "                    torch.save(net.state_dict(), 'RNN_GRU.pt')\n",
    "                valid_loss_min = val_loss.item()\n",
    "\n",
    "        net.train()\n",
    "        valid_losses.append(np.mean(val_losses))\n",
    "        train_losses.append(np.mean(train_loss))\n",
    "        val_acc.append(num_correct/(len(valid_loader)*batch_size))\n",
    "        print('Epoch: {}/{} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(e+1,epochs,np.mean(train_loss),np.mean(val_losses)))\n",
    "    return train_losses,valid_losses,tr_acc,val_acc,best_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f7f4a78c-9568-4b03-905b-e41dd00c79e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs     = 50\n",
    "input_dim  = 300\n",
    "hidden_dim = 32\n",
    "output_dim = 1\n",
    "n_layers   = 2\n",
    "lr         = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb106ebb-8426-416c-ada2-830c039d2ed3",
   "metadata": {},
   "source": [
    "### Training with Attention Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "57404114-18e9-4aa3-bd8d-083439bb501f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRU_RNN(\n",
      "  (gru): GRU(300, 32, num_layers=2, batch_first=True, dropout=1e-06)\n",
      "  (linear): Linear(in_features=32, out_features=1, bias=True)\n",
      "  (attention): Attention()\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      "  (func): Softmax(dim=-1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model       = GRU_RNN(input_dim, output_dim, hidden_dim, seq_length, n_layers)\n",
    "optimizer   = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "criterion   = torch.nn.MSELoss()#nn.CrossEntropyLoss()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3fba8d73-163a-442f-80ca-03fa23f00d11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (inf --> 0.257673).  Saving model ...\n",
      "Epoch: 1/50 \tTraining Loss: 0.286488 \tValidation Loss: 0.257673\n",
      "Epoch: 2/50 \tTraining Loss: 0.280433 \tValidation Loss: 0.316165\n",
      "Validation loss decreased (0.257673 --> 0.251178).  Saving model ...\n",
      "Epoch: 3/50 \tTraining Loss: 0.274186 \tValidation Loss: 0.251178\n",
      "Epoch: 4/50 \tTraining Loss: 0.267696 \tValidation Loss: 0.297428\n",
      "Epoch: 5/50 \tTraining Loss: 0.261043 \tValidation Loss: 0.315165\n",
      "Validation loss decreased (0.251178 --> 0.205194).  Saving model ...\n",
      "Epoch: 6/50 \tTraining Loss: 0.254432 \tValidation Loss: 0.205194\n",
      "Epoch: 7/50 \tTraining Loss: 0.248019 \tValidation Loss: 0.273160\n",
      "Epoch: 8/50 \tTraining Loss: 0.241906 \tValidation Loss: 0.210738\n",
      "Validation loss decreased (0.205194 --> 0.146092).  Saving model ...\n",
      "Epoch: 9/50 \tTraining Loss: 0.236110 \tValidation Loss: 0.146092\n",
      "Epoch: 10/50 \tTraining Loss: 0.230786 \tValidation Loss: 0.192430\n",
      "Epoch: 11/50 \tTraining Loss: 0.225379 \tValidation Loss: 0.195375\n",
      "Epoch: 12/50 \tTraining Loss: 0.220217 \tValidation Loss: 0.180195\n",
      "Epoch: 13/50 \tTraining Loss: 0.215143 \tValidation Loss: 0.211803\n",
      "Epoch: 14/50 \tTraining Loss: 0.210270 \tValidation Loss: 0.188610\n",
      "Epoch: 15/50 \tTraining Loss: 0.205536 \tValidation Loss: 0.202779\n",
      "Epoch: 16/50 \tTraining Loss: 0.200807 \tValidation Loss: 0.212532\n",
      "Epoch: 17/50 \tTraining Loss: 0.196173 \tValidation Loss: 0.218790\n",
      "Epoch: 18/50 \tTraining Loss: 0.191852 \tValidation Loss: 0.197626\n",
      "Epoch: 19/50 \tTraining Loss: 0.187407 \tValidation Loss: 0.164204\n",
      "Epoch: 20/50 \tTraining Loss: 0.183197 \tValidation Loss: 0.159777\n",
      "Epoch: 21/50 \tTraining Loss: 0.179030 \tValidation Loss: 0.178199\n",
      "Epoch: 22/50 \tTraining Loss: 0.174925 \tValidation Loss: 0.188819\n",
      "Validation loss decreased (0.146092 --> 0.142498).  Saving model ...\n",
      "Epoch: 23/50 \tTraining Loss: 0.170952 \tValidation Loss: 0.142498\n",
      "Validation loss decreased (0.142498 --> 0.092746).  Saving model ...\n",
      "Epoch: 24/50 \tTraining Loss: 0.167033 \tValidation Loss: 0.092746\n",
      "Epoch: 25/50 \tTraining Loss: 0.163186 \tValidation Loss: 0.169806\n",
      "Epoch: 26/50 \tTraining Loss: 0.159561 \tValidation Loss: 0.209635\n",
      "Epoch: 27/50 \tTraining Loss: 0.155821 \tValidation Loss: 0.177396\n",
      "Epoch: 28/50 \tTraining Loss: 0.152250 \tValidation Loss: 0.131027\n",
      "Epoch: 29/50 \tTraining Loss: 0.148791 \tValidation Loss: 0.114624\n",
      "Epoch: 30/50 \tTraining Loss: 0.145360 \tValidation Loss: 0.161044\n",
      "Epoch: 31/50 \tTraining Loss: 0.142018 \tValidation Loss: 0.139625\n",
      "Epoch: 32/50 \tTraining Loss: 0.138715 \tValidation Loss: 0.133467\n",
      "Epoch: 33/50 \tTraining Loss: 0.135485 \tValidation Loss: 0.123082\n",
      "Validation loss decreased (0.092746 --> 0.080021).  Saving model ...\n",
      "Epoch: 34/50 \tTraining Loss: 0.132414 \tValidation Loss: 0.080021\n",
      "Epoch: 35/50 \tTraining Loss: 0.129426 \tValidation Loss: 0.125307\n",
      "Epoch: 36/50 \tTraining Loss: 0.126262 \tValidation Loss: 0.102738\n",
      "Epoch: 37/50 \tTraining Loss: 0.123493 \tValidation Loss: 0.141353\n",
      "Epoch: 38/50 \tTraining Loss: 0.120603 \tValidation Loss: 0.136588\n",
      "Epoch: 39/50 \tTraining Loss: 0.117775 \tValidation Loss: 0.137666\n",
      "Epoch: 40/50 \tTraining Loss: 0.115090 \tValidation Loss: 0.140739\n",
      "Epoch: 41/50 \tTraining Loss: 0.112334 \tValidation Loss: 0.136764\n",
      "Epoch: 42/50 \tTraining Loss: 0.109801 \tValidation Loss: 0.129736\n",
      "Epoch: 43/50 \tTraining Loss: 0.107244 \tValidation Loss: 0.124063\n",
      "Epoch: 44/50 \tTraining Loss: 0.104780 \tValidation Loss: 0.114008\n",
      "Validation loss decreased (0.080021 --> 0.067614).  Saving model ...\n",
      "Epoch: 45/50 \tTraining Loss: 0.102341 \tValidation Loss: 0.067614\n",
      "Epoch: 46/50 \tTraining Loss: 0.100035 \tValidation Loss: 0.147952\n",
      "Epoch: 47/50 \tTraining Loss: 0.097631 \tValidation Loss: 0.110288\n",
      "Epoch: 48/50 \tTraining Loss: 0.095388 \tValidation Loss: 0.133669\n",
      "Epoch: 49/50 \tTraining Loss: 0.093171 \tValidation Loss: 0.102219\n",
      "Epoch: 50/50 \tTraining Loss: 0.091158 \tValidation Loss: 0.070159\n"
     ]
    }
   ],
   "source": [
    "train_losses,valid_losses,tr_acc,val_acc,best_epoch = train(epochs,train_loader,model,valid_loader,optimizer,criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8ffdb1-6e77-41c0-a430-8d81ddab3ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x     = [i for i in range(1,epochs+1)]\n",
    "xi    = [i for i in range(0,epochs+5,5)]\n",
    "xi[0] = 1\n",
    "\n",
    "f = plt.figure()\n",
    "f.set_figwidth(20)\n",
    "f.set_figheight(5)\n",
    "\n",
    "plt.plot(x,train_losses)\n",
    "plt.plot(x,valid_losses)\n",
    "plt.axvline(best_epoch, color='black')\n",
    "plt.xlabel(\"Epochs\",fontweight=\"bold\",color = 'Black', fontsize='15', horizontalalignment='center')\n",
    "plt.ylabel(\"Loss\",fontweight=\"bold\",color = 'Black', fontsize='15', horizontalalignment='center')\n",
    "plt.title(\"Losses (with Attention)\",fontweight='bold',color = 'Black', fontsize='15', horizontalalignment='center')\n",
    "plt.legend([\"Training Loss\",\"Valid Loss\",f\"Best Epoch= {best_epoch}\"])\n",
    "plt.savefig(f\"Losses {movie}_{score}_att.png\",facecolor='white');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d451cfb4-c60d-4dad-bfd8-75f2d64f28c4",
   "metadata": {},
   "source": [
    "### Training without Attention Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2462c821-ce80-418b-ae75-c054602f075e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRU_RNN(\n",
      "  (gru): GRU(300, 32, num_layers=2, batch_first=True, dropout=1e-06)\n",
      "  (linear): Linear(in_features=32, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      "  (func): Softmax(dim=-1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model     = GRU_RNN(input_dim, output_dim, hidden_dim, seq_length, n_layers,att=False)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.MSELoss()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5dc9c774-9ab2-4dbc-bb69-a84096293ce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (inf --> 0.167020).  Saving model ...\n",
      "Epoch: 1/50 \tTraining Loss: 0.323667 \tValidation Loss: 0.167020\n",
      "Validation loss decreased (0.167020 --> 0.099445).  Saving model ...\n",
      "Epoch: 2/50 \tTraining Loss: 0.159237 \tValidation Loss: 0.099445\n",
      "Validation loss decreased (0.099445 --> 0.063748).  Saving model ...\n",
      "Epoch: 3/50 \tTraining Loss: 0.081085 \tValidation Loss: 0.063748\n",
      "Epoch: 4/50 \tTraining Loss: 0.057170 \tValidation Loss: 0.072026\n",
      "Epoch: 5/50 \tTraining Loss: 0.050083 \tValidation Loss: 0.067528\n",
      "Epoch: 6/50 \tTraining Loss: 0.040832 \tValidation Loss: 0.068751\n",
      "Validation loss decreased (0.063748 --> 0.031609).  Saving model ...\n",
      "Epoch: 7/50 \tTraining Loss: 0.034273 \tValidation Loss: 0.031609\n",
      "Epoch: 8/50 \tTraining Loss: 0.031803 \tValidation Loss: 0.051059\n",
      "Epoch: 9/50 \tTraining Loss: 0.029840 \tValidation Loss: 0.067352\n",
      "Epoch: 10/50 \tTraining Loss: 0.027361 \tValidation Loss: 0.049163\n",
      "Epoch: 11/50 \tTraining Loss: 0.025961 \tValidation Loss: 0.063297\n",
      "Epoch: 12/50 \tTraining Loss: 0.024705 \tValidation Loss: 0.045974\n",
      "Validation loss decreased (0.031609 --> 0.028233).  Saving model ...\n",
      "Epoch: 13/50 \tTraining Loss: 0.023546 \tValidation Loss: 0.028233\n",
      "Epoch: 14/50 \tTraining Loss: 0.022756 \tValidation Loss: 0.047366\n",
      "Epoch: 15/50 \tTraining Loss: 0.022124 \tValidation Loss: 0.048143\n",
      "Epoch: 16/50 \tTraining Loss: 0.021317 \tValidation Loss: 0.041395\n",
      "Epoch: 17/50 \tTraining Loss: 0.020788 \tValidation Loss: 0.056296\n",
      "Epoch: 18/50 \tTraining Loss: 0.020148 \tValidation Loss: 0.040662\n",
      "Epoch: 19/50 \tTraining Loss: 0.019668 \tValidation Loss: 0.060474\n",
      "Epoch: 20/50 \tTraining Loss: 0.019035 \tValidation Loss: 0.036490\n",
      "Epoch: 21/50 \tTraining Loss: 0.018451 \tValidation Loss: 0.050615\n",
      "Epoch: 22/50 \tTraining Loss: 0.017692 \tValidation Loss: 0.042593\n",
      "Epoch: 23/50 \tTraining Loss: 0.017361 \tValidation Loss: 0.036193\n",
      "Epoch: 24/50 \tTraining Loss: 0.016848 \tValidation Loss: 0.043882\n",
      "Epoch: 25/50 \tTraining Loss: 0.016398 \tValidation Loss: 0.048144\n",
      "Validation loss decreased (0.028233 --> 0.022774).  Saving model ...\n",
      "Epoch: 26/50 \tTraining Loss: 0.015820 \tValidation Loss: 0.022774\n",
      "Epoch: 27/50 \tTraining Loss: 0.015286 \tValidation Loss: 0.047615\n",
      "Epoch: 28/50 \tTraining Loss: 0.014578 \tValidation Loss: 0.032980\n",
      "Epoch: 29/50 \tTraining Loss: 0.014845 \tValidation Loss: 0.033604\n",
      "Epoch: 30/50 \tTraining Loss: 0.013680 \tValidation Loss: 0.054207\n",
      "Epoch: 31/50 \tTraining Loss: 0.013350 \tValidation Loss: 0.041292\n",
      "Epoch: 32/50 \tTraining Loss: 0.012615 \tValidation Loss: 0.049510\n",
      "Epoch: 33/50 \tTraining Loss: 0.012068 \tValidation Loss: 0.049801\n",
      "Epoch: 34/50 \tTraining Loss: 0.011644 \tValidation Loss: 0.051813\n",
      "Epoch: 35/50 \tTraining Loss: 0.011507 \tValidation Loss: 0.025887\n",
      "Epoch: 36/50 \tTraining Loss: 0.010915 \tValidation Loss: 0.068562\n",
      "Epoch: 37/50 \tTraining Loss: 0.010239 \tValidation Loss: 0.062498\n",
      "Epoch: 38/50 \tTraining Loss: 0.009942 \tValidation Loss: 0.051498\n",
      "Epoch: 39/50 \tTraining Loss: 0.009017 \tValidation Loss: 0.059416\n",
      "Epoch: 40/50 \tTraining Loss: 0.009033 \tValidation Loss: 0.044734\n",
      "Epoch: 41/50 \tTraining Loss: 0.008434 \tValidation Loss: 0.041929\n",
      "Epoch: 42/50 \tTraining Loss: 0.007863 \tValidation Loss: 0.029082\n",
      "Epoch: 43/50 \tTraining Loss: 0.007404 \tValidation Loss: 0.049933\n",
      "Epoch: 44/50 \tTraining Loss: 0.006944 \tValidation Loss: 0.063979\n",
      "Epoch: 45/50 \tTraining Loss: 0.006818 \tValidation Loss: 0.060668\n",
      "Epoch: 46/50 \tTraining Loss: 0.006422 \tValidation Loss: 0.076857\n",
      "Epoch: 47/50 \tTraining Loss: 0.006154 \tValidation Loss: 0.050200\n",
      "Epoch: 48/50 \tTraining Loss: 0.005820 \tValidation Loss: 0.045543\n",
      "Epoch: 49/50 \tTraining Loss: 0.005322 \tValidation Loss: 0.048257\n",
      "Epoch: 50/50 \tTraining Loss: 0.005149 \tValidation Loss: 0.046131\n"
     ]
    }
   ],
   "source": [
    "train_losses_1,valid_losses_1,tr_acc_1,val_acc_1,best_epoch = train(epochs,train_loader,model,valid_loader,optimizer,criterion,att=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7fbcaa-5291-4e60-8326-781a1c109878",
   "metadata": {},
   "outputs": [],
   "source": [
    "x     = [i for i in range(1,epochs+1)]\n",
    "xi    = [i for i in range(0,epochs+5,5)]\n",
    "f = plt.figure()\n",
    "f.set_figwidth(18)\n",
    "f.set_figheight(5)\n",
    "\n",
    "plt.plot(x,train_losses_1)\n",
    "plt.plot(x,valid_losses_1)\n",
    "plt.axvline(best_epoch, color='black')\n",
    "plt.xlabel(\"Epochs\",fontweight=\"bold\",color = 'Black', fontsize='15', horizontalalignment='center')\n",
    "plt.ylabel(\"Loss\",fontweight=\"bold\",color = 'Black', fontsize='15', horizontalalignment='center')\n",
    "plt.title(f\"Losses (without Attention)\",fontweight='bold',color = 'Black', fontsize='15', horizontalalignment='center')\n",
    "plt.legend([\"Training Loss\",\"Valid Loss\",f\"Best Epoch= {best_epoch}\"])\n",
    "plt.savefig(f\"Losses {movie}_{score}.png\",facecolor='white');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca13d955-6efa-41c1-af86-a269d3c19e65",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "274e26e7-26be-44fb-864a-29508d82fba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(test_loader,net):\n",
    "    v_c = 0\n",
    "    net.to(device)\n",
    "    net.eval()\n",
    "    num_correct = 0\n",
    "    valid_acc = 0\n",
    "    out  = []\n",
    "    l_all =[]\n",
    "    h = net.init_hidden(batch_size)\n",
    "    f = True\n",
    "    for inputs, labels in test_loader:\n",
    "        v_c += 1\n",
    "        if (v_c == 8):#12, #6, #8\n",
    "            continue\n",
    "        h = h.data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        #l_all.append(labels.cpu().detach().numpy())\n",
    "        output, h = net(inputs, h)\n",
    "        #print(output,labels)\n",
    "        #print(labels)\n",
    "        #pred = torch.round(output.squeeze()) \n",
    "        output = output.view(batch_size,-1)\n",
    "        print(output.shape,labels.shape)\n",
    "        output = output.cpu().detach().numpy()\n",
    "        labels = labels.cpu().detach().numpy()\n",
    "        if f:\n",
    "            out  = output\n",
    "            l_all = labels\n",
    "            f = False\n",
    "        else:\n",
    "            out = np.concatenate((out,output), axis=0)\n",
    "            l_all = np.concatenate((l_all,labels),axis=0)\n",
    "    return out,l_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bd9de7a4-2763-47a0-98f9-e19f14e5e7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores(opt,lbl):\n",
    "    co = []\n",
    "    po = []\n",
    "    for i in range(lbl.shape[1]):\n",
    "        z = []\n",
    "        m = []\n",
    "        for j in range(lbl.shape[0]):\n",
    "            z.append(opt[j][i])\n",
    "            m.append(lbl[j][i])\n",
    "        coef, p = spearmanr(z,m)\n",
    "        co.append(coef)\n",
    "        po.append(p)\n",
    "    return co,po"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2144d155-fedd-4a72-8329-28664e586c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#xa = DataLoader(ConcatDataset([train_data,test_data]),batch_size=batch_size)\n",
    "#xa = test_loader\n",
    "xa = [d for dl in [valid_loader, test_loader] for d in dl]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f9273c-b2bc-4934-9e2b-0e0d9a74e0d7",
   "metadata": {},
   "source": [
    "### Prediction with Attention Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e38fa47f-8036-4646-ad00-2e022d9df2a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GRU_RNN(input_dim, output_dim, hidden_dim, seq_length, n_layers)\n",
    "model.load_state_dict(torch.load('RNN_GRU_Att.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cbd08da8-e8cf-4c73-8774-5493094a744b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([15, 256]) torch.Size([15, 256])\n",
      "torch.Size([15, 256]) torch.Size([15, 256])\n",
      "torch.Size([15, 256]) torch.Size([15, 256])\n",
      "torch.Size([15, 256]) torch.Size([15, 256])\n",
      "torch.Size([15, 256]) torch.Size([15, 256])\n",
      "torch.Size([15, 256]) torch.Size([15, 256])\n",
      "torch.Size([15, 256]) torch.Size([15, 256])\n"
     ]
    }
   ],
   "source": [
    "out,l_all = test(xa,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "82876054-8b3e-4d97-b073-3690dbedfaa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "coef, p_val = get_scores(out,l_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9435e9fa-56b3-4b1d-88eb-1bb5ed9ee766",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plt.figure()\n",
    "f.set_figwidth(18)\n",
    "f.set_figheight(8)\n",
    "plt.plot([x for x in range(seq_length)],coef);\n",
    "#plt.plot([x for x in range(seq_length)],coef1);\n",
    "plt.axhline(y = max(coef), color = 'g', linestyle = '-')\n",
    "plt.axhline(y = min(coef), color = 'r', linestyle = '-')\n",
    "plt.ylim([min(coef)-0.05, max(coef)+0.05])\n",
    "plt.grid()\n",
    "plt.title(f\"{score}: {movie}\")\n",
    "plt.xlabel(\"Time Points\",fontweight=\"bold\",color = 'Black', fontsize='15', horizontalalignment='center')\n",
    "plt.ylabel(\"Spearman correlation\",fontweight=\"bold\",color = 'Black', fontsize='15', horizontalalignment='center')\n",
    "plt.legend([\"Predected Spearman\",f\"Max: {round(max(coef),4)}\",f\"Min: {round(min(coef),4)}\"])\n",
    "\n",
    "plt.savefig(f\"Spearman {movie}_{score}_att_{shuffle}.png\",facecolor='white');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb34194-15b1-41fb-b326-ad3d5e1e8aab",
   "metadata": {},
   "source": [
    "f = plt.figure()\n",
    "f.set_figwidth(20)\n",
    "f.set_figheight(8)\n",
    "plt.plot([x for x in range(seq_length)],coef);\n",
    "plt.axhline(y = max(coef), color = 'g', linestyle = '-')\n",
    "plt.axhline(y = min(coef), color = 'r', linestyle = '-')\n",
    "plt.ylim(max(coef), min(coef));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d52f2a7-eb3e-4b5b-8450-2b105fc9ef0c",
   "metadata": {},
   "source": [
    "### Prediction without Attention Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "827116b1-dd37-4aa3-822e-c8f4fd00af19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GRU_RNN(input_dim, output_dim, hidden_dim, seq_length, n_layers,att=False)\n",
    "model.load_state_dict(torch.load('RNN_GRU.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f62549f3-9c69-467c-8eb1-d8ad2af50710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([15, 256]) torch.Size([15, 256])\n",
      "torch.Size([15, 256]) torch.Size([15, 256])\n",
      "torch.Size([15, 256]) torch.Size([15, 256])\n",
      "torch.Size([15, 256]) torch.Size([15, 256])\n",
      "torch.Size([15, 256]) torch.Size([15, 256])\n",
      "torch.Size([15, 256]) torch.Size([15, 256])\n",
      "torch.Size([15, 256]) torch.Size([15, 256])\n"
     ]
    }
   ],
   "source": [
    "out_1,l_all_1 = test(xa,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4211374a-62db-4832-b7e6-e40cfd432d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_1, p_val_1 = get_scores(out_1,l_all_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae3d626-1fcb-4d51-a32e-3727c05b0131",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plt.figure()\n",
    "f.set_figwidth(18)\n",
    "f.set_figheight(8)\n",
    "plt.plot([x for x in range(seq_length)],coef_1);\n",
    "plt.axhline(y = max(coef_1), color = 'g', linestyle = '-')\n",
    "plt.axhline(y = min(coef_1), color = 'r', linestyle = '-')\n",
    "plt.ylim([min(coef_1)-0.05, max(coef_1)+0.05])\n",
    "plt.title(f\"{score}: {movie.capitalize()}\")\n",
    "plt.xlabel(\"Time Points\",fontweight=\"bold\",color = 'Black', fontsize='15', horizontalalignment='center')\n",
    "plt.ylabel(\"Spearman correlation\",fontweight=\"bold\",color = 'Black', fontsize='15', horizontalalignment='center')\n",
    "plt.legend([\"Predected Spearman\",f\"Max: {round(max(coef),4)}\",f\"Min: {round(min(coef),4)}\"])\n",
    "plt.grid()\n",
    "plt.legend([\"Predected Spearman\",f\"Max: {round(max(coef_1),4)}\",f\"Min: {round(min(coef_1),4)}\"])\n",
    "plt.savefig(f\"Spearman {movie}_{score}_{shuffle}.png\",facecolor='white');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e446fbc1-2a24-4fb2-b9b6-cc3b04c88420",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8201b9ba-5bbe-41ba-9cdd-78f5386140a5",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Spearman Correlation Rank"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6b9df1-bec6-4130-a8dc-dc1600fb8f94",
   "metadata": {},
   "source": [
    "#### Starwars || Verbal IQ\n",
    "<h3><center> Attention Model </center></h3>\n",
    "<img src=\"Spearman starwars_PicVocab_Unadj_att_False.png\">\n",
    "<h3><center> Model in Paper</center></h3>\n",
    "<img src=\"Spearman starwars_PicVocab_Unadj_False.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171830b4-6d2d-46e4-bb3c-5c18b31a1a4f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a02fa6-fd63-46be-895a-020bd2cba79b",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Starwars || Fluid Intelligence\n",
    "<h3> <center> Attention Model </center></h3>\n",
    "<img src=\"Spearman starwars_PMAT24_A_CR_att_False.png\">\n",
    "<h3><center> Model in Paper</center></h3>\n",
    "<img src=\"Spearman starwars_PMAT24_A_CR_False.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a494b3-b382-44f6-bca6-0520a1ce35eb",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf4b4e2-a5ef-4475-97ce-e7a50197b6d6",
   "metadata": {},
   "source": [
    "#### Starwars || Verbal IQ\n",
    "<h3><center> Attention Model </center></h3>\n",
    "<img src=\"Losses starwars_PicVocab_Unadj_att.png\">\n",
    "<h3><center> Model in Paper</center></h3>\n",
    "<img src=\"Losses starwars_PicVocab_Unadj.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b312fe72-4908-4d23-a18e-56b4836d6c2c",
   "metadata": {},
   "source": [
    "#### Starwars || Fluid Intelligence\n",
    "<h3> <center> Attention Model </center></h3>\n",
    "<img src=\"Losses starwars_PMAT24_A_CR_att.png\">\n",
    "<h3><center> Model in Paper</center></h3>\n",
    "<img src=\"Losses starwars_PMAT24_A_CR.png\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
